---
title: "XGBoost Vignette: Fraud Detection"
author: "Mu Niu, Yixin Xue, Amber Wang, David Lin, Qifei Cui"
date: "2023-12-08"
output: html_document
---

#### Objectives

In this vignette, you'll be guided through a comprehensive journey from Exploratory Data Analysis(EDA) to employing XGBoost, a cutting-edge and industry-standard model. You'll acquire skills in:

* Utilizing functions from the **ggplot** and **corrplot** packages for EDA and data visualization. These tools are essential for understanding the underlying patterns and relationships in your data.

* Employing functions from the **xgboost** package to train an Extreme Gradient Boosting Model. This powerful machine learning technique is renowned for its efficiency and effectiveness in various predictive modeling tasks.

* Leveraging functions from the **pROC** package to evaluate the performance of your model. This is crucial for assessing the effectiveness of your predictive model in real-world scenarios.

By the end of this vignette, you will have a solid understanding of how to effectively use these tools in a cohesive workflow to build and assess a robust machine learning model.


#### Prerequisites

First, please follow the action item below to get set up for the vignette. You may need to install one or more packages if the library() calls return an error.
```{r, message = FALSE, warning = FALSE}
# load packages
library(tidymodels)
library(xgboost)
library(tidyverse)
library(pROC)
library(ggplot2)
library(corrplot)
# read data
data <- read_csv("data/card_transdata.csv") 
```

## Abstract

XGBoost, an acronym for eXtreme Gradient Boosting, was crafted by Dr. Tianqi Chen in 2016 as a gradient-boosting algorithm. Demonstrating remarkable success, it has emerged as a formidable player in various data science competitions. Renowned for its exceptional performance, XGBoost stands out as a robust solution for tasks such as handling structured data, text classification, regression, and notably, exhibiting superior capabilities on large-scale datasets. Our vignette presents a comprehensive exploration of XGBoost, including its mathematical principles, programming language support, and practical application in classification problems.

## Dataset Introduction

Our vignette is conducted on a simulated ["Credit Card Fraud"](https://www.kaggle.com/datasets/dhanushnarayananr/credit-card-fraud/data) dataset obtained on Kaggle. This dataset presents a binary classification problem, classifying transactions as either fraudulent or legitimate. With 8 variables and a total of 1,000,000 observations, only 8.74% (87,403) are marked as fraudulent, highlighting a significant class imbalance.

```{r, echo=FALSE, message= F}
head(data) %>% knitr::kable()
```

The variable explanations are as follows:

|Variable Name|   Feature Explanation   | Type |
|:-------------:|:--------------:|:-----:|
|distance_from_home|  The distance from home where the transaction happened  | Continuous |
|distance_from_last_transaction    |    The distance from last transaction happened  | Continuous |
|ratio_to_median_purchase_price    |    Ratio of purchased price transaction to median purchase price| Continuous |
|repeat_retailer | Is the transaction happened from same retailer| Discrete |
|used_chip | Is the transaction through credit card chip | Discrete |
|used_pin_number | Is the transaction happened by using PIN number| Discrete |
|online_order | Is the transaction an online order| Discrete |
|fraud | Is the transaction fraudulent| Discrete |

## Exploratory Data Analysis

First, we want to check if there is missing values in the dataset

```{r}
data %>% is.na() %>% sum()
```

Through the code above, we know that this dataset contains no missing value, therefore there is no imputation needed. Then, we can use the following chunk to check the distribution of the variables.

```{r, message = F}
par(mfrow = c(3, 3))
for (col in colnames(data)) {
  density_values <- density(data[[col]])
  plot(density_values, main = col, col = "skyblue", lwd = 2)
}
```

We can observe from the density curve graph of each variable that except for the first three, the other 5 variables including our response variable 'fraud' are binary. But the first three variables are greatly skewed. After that, we want to see if correlation exists between the variables.

```{r, message = F}
par(mfrow = c(1, 1))
# Correlation
data %>%
  cor(use = 'everything') %>%
  corrplot(type = 'lower')
```

From the correlation plot, the numerical variable 'ratio_to_median' has a positive correlation with respect to 'fraud', and then are 'distance_from_home' and 'online_order'. Other than that, 'used_pin_number' has negatively correlated with 'fraud'. Lastly, we want to specifically analyze the distribution of the response variable, which is our most interested variable.

```{r, message = F}
data %>%
  ggplot(aes(x = fraud)) +
  geom_bar() +
  labs(title = "Fraud Distribution", x = 'fraud', y = 'count') +
  theme(plot.title = element_text(hjust = 0.5))
```

This graph represents the distribution of 'fraud'. Around 91% of the transactions are not fraud and 9% are fraud, which reflects a imbalance in the data. This imbalance suggests adapting performance metrics like AUC-ROC, f1 score as evaluation metrics instead of "accurary" to assess the ML model.


## Data Preprocessing

In preparation for modeling, the dataset undergoes initial processing steps. Outliers are systematically trimmed, and selected variables are renamed for clarity and conciseness. Subsequently, the mutate function is employed to enact transformations, notably generating scaled representations for the columns 'dist_home,' 'dist_last_transact,' and 'ratio_to_med_price.'

```{r}
trim <- function(x, .at){
  x[abs(x) > .at] <- sign(x[abs(x) > .at])*.at
  return(x)
}

# rename and scale: since some variable are on distance scale/some are ratio
data <- data %>%
  rename(dist_home = distance_from_home,
         dist_last_transact = distance_from_last_transaction,
         ratio_to_med_price = ratio_to_median_purchase_price) %>%
  mutate(across(.cols = c(dist_home, dist_last_transact, ratio_to_med_price), 
                ~ trim(scale((.x))[, 1], .at = 3)))
```

## Introduction to Boosted Trees and XGBoost

### Gradient Boosted Decision Tree (GBDT)

In GBDT, the prediction of observation i is the sum of the predicted values from each of the k trees in the ensemble.

$$\hat{y} = \sum_{i=1}^{k} f_k(x_i)$$Each tree in the boosting model fits the negative gradient of the loss function to the current model's prediction, and the current model's prediction is the sum of the first k-1 trees.

If we use mean squared error as the loss function in a regression task, each tree simply fits the residual.

For example, suppose a person is 30 years old, and a GBDT model is used to predict his age.

-   Given the dataset, suppose the first tree's prediction is 20.

-   Then the second tree fits the residual, which is 30 - 20 = 10. Suppose the second tree outputs 7.

-   Then the third tree fits the residual 10 - 7 = 3. Suppose the third tree outputs 2.

-   By adding the prediction of the three trees, we get our final prediction.

### XGBoost

XGBoost is an efficient and scalable implementation of the gradient-boosted decision tree.

#### Tree Boosting

The objective function for k-th tree is $$\sum_{i=1}^{n} l(y_i, \hat{y_i}^{(k)})+\sum_{i=1}^{K}\omega(f_i)$$.

The first term in the objective function denotes the loss function. It can be MSE in regression or cross-entropy in classification problems. The second term $\omega(f_i)$ is a regularized term to control the model's complexity to avoid overfitting.

When additively training the model, XGBoost used a second-order Taylor expansion to approximate the objective function instead of using the negative gradient.

After derivation, the optimization goal at step k will be $$\sum_{i=1}^{n}[g_if_k(x_i) + \frac{1}{2}h_{i}f_k^2(x_i)] + \omega(f_k)$$,

where $g_i$ and $h_i$ denote the first-order and second-order derivative of the loss function to the current model's prediction. i.e. $g_i = \partial_{\hat{y_i}^{k-1}}l(y_i, \hat{y_i}^{k-1})$ and $h_i = \partial^2_{\hat{y_i}{k-1}}l(y_i, \hat{y_i}^{k-1})$

We find that the objective function for each tree depends only on $g_i$ and $h_i$, derivatives of the loss function. Therefore, XGBoost can be widely used in many scenarios, including regression, classification, and ranking problems, as long as XGBoost is given a loss function that is second-order differentiable. You can even customize your loss function in XGBoost!

### Approximate Split Finding

We first look at a basic greedy approach for finding best split point.

![](img/Exact_greedy_algorithm.png)

In the above Exact Greedy Algorithm, we need to traverse every possible value of every feature to find the best split point. The score measures the reduction of loss function before and after the split. However, the method is inefficient when dealing with large datasets.

Instead, XGBoost used an approximated approach to find split points efficiently.

![](img/approximate_algorithm.webp)

In the approximate framework, we first select some candidate splitting points according to percentiles of the feature distribution based on some criteria. For instance, a sorted feature column contains 10 values \[1,2,2,2,2,4,5,7,9,20\]. A 0.6 quantile corresponds to 4.

### Weighted Quantile Sketch

How do we find these candidate split points? We cannot assign equal weights to every observation of the feature, as larger values may have a more significant impact on the loss function. Therefore, we need to calculate a weight for each feature value to make candidates distribute evenly on the data.

For each observation, XGBoost used its second-order derivative $h_i$ as weight. Formally, let multi-set $D_k = {(x_{1k},h_1), (x_{2k}, h_2)â€¦(x_{nk}, h_{n})}$ denote k-th feature value and its corresponding second-order gradient, the rank function is $$r_k(z) = \frac{1}{\sum_{(x,h) \in D_k}h}\sum_{(x,h) \in D_k, x < z}h$$.

For example, given

| feature value | 1   | 4   | 6   | 10  | 12  | 20  | 30  | 44  | 59  |
|---------------|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| $$h_i$$       | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.1 | 0.4 | 0.2 | 0.6 |

30 corresponds to $\frac{1}{3}$ percentile, and 59 corresponds to $\frac{2}{3}$ percentile.

### Other techniques in XGBoost to improve efficiency

-   Each column (feature) of the dataset is pre-sorted and stored in memory as a column block. Each block contains the sorted feature column and each observation's corresponding label value. Recall that when finding the best split point in a decision tree, samples with the feature value less than the split value go to the left subtree, and samples with the feature value larger than the split point go to the right subtree. Instead of sorting in every split in every tree, this pre-sorting step is done only once. Then the efficiently sorted columns will be reused many times in building every single tree.

-   XGBoost supports parallel computing during the process of finding the best split point in building a single tree. Then the information gain calculation for each feature can be done in parallel. Moreover, You can tune the parameter nthread to control the number of cores used in training the XGBoost model.

-   XGBoost supports column subsampling similar to a random forest.

## Fraud Detection with XGBoost in R

### Data Preparation

```{r}
#set seed for reproducibility
set.seed(197)
#split training/validation/testing
data.split <- data %>% 
  initial_split(prop = 0.6, strata = fraud)

test.split <- initial_split(testing(data.split), prop = 0.5, strata = fraud)
data.val <- training(test.split)
data.test <- testing(test.split)

#define predictor and response variables in training set
# NOTE: XGBoost only use matrix data
train.x <-  data.matrix(training(data.split) %>% select(-fraud))
train.y <-  training(data.split) %>% pull(fraud)

#define predictor and response variables in validation set
val.x <-  data.matrix(data.val %>% select(-fraud))
val.y <-  data.val %>% pull(fraud)

#define predictor and response variables in testing set
test.x <-  data.matrix(data.test %>% select(-fraud))
test.y <-  data.test %>% pull(fraud)
```

### Model Fitting

-   objective: What's your goal? In this case, we use 'binary:logistic' to predict the probability of having a fraud based on the predictor variables

-   eta: How fast to learn? default value 0.3

-   data: Training data in DMatrix structure

-   max.depth: The size of each tree and a rule of thumb is to use 2 or 3 to prevent overfitting

-   watchlist: Track model performance on training and validation data during the training process to prevent overfitting

-   nrounds: number of boosting rounds

-   early_stopping_rounds: Stop the training process if the model's accuracy on validation set hasn't improved for the specified number of rounds

```{r}
#define xgb.DMatirx: This is a specialized data structure that xgboost uses for efficiency
xgb.train <-  xgb.DMatrix(data = train.x, label = train.y)
xgb.val <-  xgb.DMatrix(data = val.x, label = val.y)
xgb.test <-  xgb.DMatrix(data = test.x, label = test.y)
```

Here we use a watchlist and a validation set to select the optimal number of boosting rounds(nrounds). When training, we track the performance of the model on the training and validation datasets. Watchlist is usually used with the early_stopping_rounds parameter to determine when the model starts to overfit and we should stop.

```{r}
watchlist = list(train=xgb.train, validation=xgb.val)

params <- list(
  objective = "binary:logistic", # loss function for binary classification problem
  eta = 0.3 # learning rate
)

#fit XGBoost model and display training and testing data at each round
model <-  xgb.train(params = params, 
                    data = xgb.train, # Training data
                    max.depth = 3, # Size of each individual tree
                    watchlist=watchlist, # Track model performance on train and validation set
                    nrounds = 500, # Number of boosting iterations
                    early_stopping_rounds = 50) # Number of iterations we will wait for the next decrease
```

## Results and Analysis

Our model stops boosting at 261th round, which means the model accuracy decreases in the next 50 rounds. So we know that overfitting occurs after that. Hence, we changed parameter nrounds to 216 to train our final model.

Similar to the random forest, we can visualize the variable importance in XGBoost.

```{r}
# Define final model
# The argument verbose = 0 tells R not to display the training and testing error for each round.
final <-  xgboost(params = params, data = xgb.train, max.depth = 3, nrounds = 216, verbose = 0)

### Feature Importance
# WRITE-UP refer to: https://cran.r-project.org/web/packages/xgboost/vignettes/discoverYourData.html
importance <- xgb.importance(feature_names = colnames(train.x), model = final)
head(importance)

# plot
# 3 most important features
xgb.plot.importance(importance_matrix = importance, top_n = 5)
```

### Prediction and Accuracy Measure

```{r}
# Use model to make predictions on test data
pred.y <- predict(final, test.x)

# Label test data according to the predicted probability
pred.label <- ifelse(pred.y > 0.5, 1, 0)

# Confusion Matrix
confusion.matrix <- table(Predicted = pred.label, Actual = test.y)
print(confusion.matrix)

# AUC-ROC
roc <- roc(test.y, pred.label)
auc <- auc(roc)
print(auc)

# Visualization
ggroc(roc) +
  labs(title = "ROC Curve", x = "False Positive Rate", y = "True Positive Rate") + 
  annotate("text", x = 0.2, y = 0.8, label = paste("AUC =", round(auc, 5)))
```

The plot on the right shows a very high AUC score of 0.99988, indicating that the model is almost a perfect classifier. Such an unusual result is probably because the simulated dataset is designed for practice purposes, and the data is so unbalanced.
